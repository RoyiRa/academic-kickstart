@article{10.1162/tacl_a_00410,
    author = {Elazar, Yanai and Kassner, Nora and Ravfogel, Shauli and Ravichander, Abhilasha and Hovy, Eduard and SchÃ¼tze, Hinrich and Goldberg, Yoav},
    title = "{Measuring and Improving Consistency in Pretrained Language Models}",
    journal = {Transactions of the Association for Computational Linguistics},
    volume = {9},
    pages = {1012-1031},
    year = {2021},
    month = {12},
    abstract = "{Consistency of a modelâ€”that is, the invariance of its behavior under meaning-preserving alternations in its inputâ€”is a highly desirable property in natural language processing. In this paper we study the question: Are Pretrained Language Models (PLMs) consistent with respect to factual knowledge? To this end, we create ParaRelðŸ¤˜, a high-quality resource of cloze-style query English paraphrases. It contains a total of 328 paraphrases for 38 relations. Using ParaRelðŸ¤˜, we show that the consistency of all PLMs we experiment with is poorâ€” though with high variance between relations. Our analysis of the representational spaces of PLMs suggests that they have a poor structure and are currently not suitable for representing knowledge robustly. Finally, we propose a method for improving model consistency and experimentally demonstrate its effectiveness.1}",
    issn = {2307-387X},
    doi = {10.1162/tacl_a_00410},
    url = {https://doi.org/10.1162/tacl\_a\_00410},
    eprint = {https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl\_a\_00410/1975957/tacl\_a\_00410.pdf},
}



