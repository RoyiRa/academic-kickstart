@inproceedings{jacovi-etal-2021-contrastive,
    title = "Contrastive Explanations for Model Interpretability",
    author = "Jacovi, Alon  and
      Swayamdipta, Swabha  and
      Ravfogel, Shauli  and
      Elazar, Yanai  and
      Choi, Yejin  and
      Goldberg, Yoav",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.120",
    doi = "10.18653/v1/2021.emnlp-main.120",
    pages = "1597--1611",
    abstract = "Contrastive explanations clarify why an event occurred in contrast to another. They are inherently intuitive to humans to both produce and comprehend. We propose a method to produce contrastive explanations in the latent space, via a projection of the input representation, such that only the features that differentiate two potential decisions are captured. Our modification allows model behavior to consider only contrastive reasoning, and uncover which aspects of the input are useful for and against particular decisions. Our contrastive explanations can additionally answer for which label, and against which alternative label, is a given input feature useful. We produce contrastive explanations via both high-level abstract concept attribution and low-level input token/span attribution for two NLP classification benchmarks. Our findings demonstrate the ability of label-contrastive explanations to provide fine-grained interpretability of model decisions.",
}

